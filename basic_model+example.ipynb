{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd38edc0-587f-4a23-81c7-2b6e1f2d95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#back to the main problem\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "vsig = np.vectorize(sigmoid)\n",
    "\n",
    "def dsig(x):\n",
    "    return np.exp(-x)/((1+np.exp(-x))**2)\n",
    "vdsig = np.vectorize(dsig)\n",
    "\n",
    "def dtanh(x):\n",
    "    4*np.exp(-2*x)/((1+np.exp(-2*x))**2)\n",
    "vdtanh = np.vectorize(dtanh)\n",
    "\n",
    "def softmax(v): \n",
    "    return np.exp(v)/sum(np.exp(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "347af51e-2929-473c-bab2-6689af499041",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the basic dense layer class\n",
    "class denseLayer:\n",
    "    def __init__(self, linear, bias=0, activation=None):\n",
    "        self.linear = linear\n",
    "        self.activation = activation #string\n",
    "        self.bias = bias\n",
    "        \n",
    "    #returns a 1-dim'l np array representing the diagonal of the derivative of the activation\n",
    "    #evaluated at the input vector\n",
    "    def activationDerDiag(self,input_vect): #input_vect a column vector to be fed into the derivative\n",
    "        if self.activation == None:\n",
    "            return np.ones(len(input_vect))\n",
    "        elif self.activation.lower() == 'relu':\n",
    "            #i guess this should return 1 or 0 depending on whether input is >0\n",
    "            for i in range(0,len(input_vect)):\n",
    "                if input_vect[i]>0:\n",
    "                    input_vect[i] = 1\n",
    "                else: input_vect[i] = 0\n",
    "            return np.array(input_vect)\n",
    "        elif self.activation.lower() == 'sigmoid':\n",
    "            return vdsig(input_vect)\n",
    "        elif self.activation.lower() == 'tanh':\n",
    "            return vdtanh(input_vect)\n",
    "        elif self.activation.lower() == 'softmax':\n",
    "            #this is not the actual derivative of the diagonal for softmax\n",
    "            #i am currently assuming softmax is only going to appear at the end of a network, where it's \n",
    "            #used with cross entropy loss and the gradient is already incorporated in the gradient of the loss\n",
    "            return np.ones(len(input_vect))\n",
    "        else:\n",
    "            raise ValueError('The activation you are trying to use has not yet been implemented. We have relu, sigmoid, tanh, and softmax.')\n",
    "        \n",
    "    #forward pass of a layer without the activation function\n",
    "    def linearPass(self, input_vector, add_bias = True):\n",
    "        if add_bias == False:\n",
    "            return self.linear@input_vector\n",
    "        return (self.linear@input_vector + self.bias)\n",
    "        \n",
    "    #if activation == None, this is the same as linearPass\n",
    "    def forwardPass(self,input_vector):\n",
    "        if self.activation == None:\n",
    "            return (self.linear@input_vector + self.bias)\n",
    "        elif self.activation.lower() == 'relu':\n",
    "            return np.maximum(0,self.linearPass(input_vector))\n",
    "        elif self.activation.lower() == 'sigmoid':\n",
    "            return vsig(self.linearPass(input_vector))\n",
    "        elif self.activation.lower() == 'tanh':\n",
    "            return np.tanh(self.linearPass(input_vector))\n",
    "        elif self.activation.lower() == 'softmax':\n",
    "            return softmax(self.linearPass(input_vector))\n",
    "        else:\n",
    "            raise ValueError('The activation you are trying to use has not yet been implemented. We have relu, sigmoid, tanh, and softmax.')\n",
    "\n",
    "#the basic neural network class, which just allows for a sequence of dense layers\n",
    "class nNet:\n",
    "    def __init__(self,layersList):\n",
    "        self.layersList = layersList #a list of layers whose outputs match the following inputs\n",
    "        \n",
    "    def forwardPass(self,input_vector, return_all_steps = True):\n",
    "        #this case is for when training is done\n",
    "        if return_all_steps == False:\n",
    "            for layer in self.layersList:\n",
    "                input_vector = layer.forwardPass(input_vector)\n",
    "            return input_vector\n",
    "        \n",
    "        #keeping track of the output after each layer to save time during backprop\n",
    "        allStepsOut = [input_vector]\n",
    "        for layer in self.layersList:\n",
    "            input_vector = layer.forwardPass(input_vector)\n",
    "            allStepsOut.append(input_vector)\n",
    "        return allStepsOut\n",
    "        \n",
    "    #data will be list of pairs of column vectors (x,y)\n",
    "    def backprop(self, data, loss, learning_rate, batch_size = 1): #this automatically does \"stochastic gradient descent\"\n",
    "        for pair in data:\n",
    "#            print(f'forward passing now. x is {pair[0]} and y is {pair[1]}')\n",
    "            xForward = self.forwardPass(pair[0])\n",
    "#            print(f'the entire forward pass of x is {xForward}')\n",
    "            if loss.lower() == 'mse': #mean squared error\n",
    "                rtTimesGradEtc = learning_rate*2*np.atleast_2d(xForward[-1]-pair[1]).T #T to make row vector from column\n",
    "#                print(f'the gradient is starting at {rtTimesGradEtc}')\n",
    "            elif loss.lower() == 'cel': #cross entropy loss - last layer MUST be softmax\n",
    "                if self.layersList[-1] != 'softmax':\n",
    "                    raise ValueError('This implementation of cross-entropy loss requires that the last layer of the network is a softmax')\n",
    "                rtTimesGradEtc = learning_rate*np.atleast_2d(xForward[-1] - pair[1]).T #is it really this simple?\n",
    "            else:\n",
    "                raise ValueError('The loss you are trying to use has not yet been implemented. We have cel, mse, and ???.')\n",
    "            #\n",
    "            #\n",
    "            #\n",
    "            k=len(self.layersList)\n",
    "            for i in range (0,k):\n",
    "#                print(f'backpropagating {i} steps now')\n",
    "                newMtx_kminusi = self.layersList[-i-1].linear\n",
    "                newBias_kminusi = self.layersList[-i-1].bias\n",
    "                \n",
    "#                print(f'the current linear term is {newMtx_kminusi} and the current bias is {newBias_kminusi}')\n",
    "                \n",
    "                #multiply rtTimesGradEtc by derivatives of activation\n",
    "                \n",
    "                rtTimesGradEtc = rtTimesGradEtc*self.layersList[-i-1].activationDerDiag( self.layersList[-i-1].linear @ xForward[-i-2]+self.layersList[-i-1].bias)\n",
    "                \n",
    "#                print(f'updating the gradient by entrywise multiplication with {self.layersList[-i-1].activationDerDiag(self.layersList[-i-1].linear @ xForward[-i-2]+self.layersList[-i-1].bias)}')\n",
    "                \n",
    "                #using matrix equations to store the new layer parameters\n",
    "                newMtx_kminusi = self.layersList[-i-1].linear - (np.atleast_2d(rtTimesGradEtc).T @ xForward[-i-2])\n",
    "                newBias_kminusi = self.layersList[-i-1].bias - np.atleast_2d(rtTimesGradEtc).T\n",
    "                \n",
    "#                print(f'updating the linear term to {newMtx_kminusi} and the bias to {newBias_kminusi}')\n",
    "                \n",
    "                #finishing our gradient update\n",
    "                rtTimesGradEtc = rtTimesGradEtc @ self.layersList[-i-1].linear\n",
    "                \n",
    "#                print(f'updating the gradient by matrix multiplication with {self.layersList[-i-1].linear}')\n",
    "                \n",
    "                #updating the linear and bias term in the actual model\n",
    "                self.layersList[-i-1].linear = newMtx_kminusi\n",
    "                self.layersList[-i-1].bias = newBias_kminusi\n",
    "                \n",
    "    def nEpochs(self, n, data, loss, learning_rate, batch_size = 1):\n",
    "        for _ in range(0,n):\n",
    "#            print(f'starting epoch {_}')\n",
    "            self.backprop(data, loss, learning_rate, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fce52e4-b3fb-42ae-9db9-892ef9a68b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# as a first test, let's do some least squares - let the data be the points (0,1) and (1,2) in R^2\n",
    "#\n",
    "data = [(np.array([[0]]),np.array([[1]])),(np.array([[1]]),np.array([[2]]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "724e7343-3b35-496c-ba03-244816a8aaaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.46972727]] [[-2.73896382]]\n"
     ]
    }
   ],
   "source": [
    "#let's start our single layer model as a random linear function plus a random bias\n",
    "import random \n",
    "\n",
    "layer = denseLayer(np.array([[random.uniform(-10,10)]]),np.array([[random.uniform(-10,10)]]))\n",
    "\n",
    "model = nNet([layer])\n",
    "\n",
    "print(layer.linear,layer.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bd83ef7-d382-4c14-92c2-f78ce6ba369c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99895523]] [[1.00065937]]\n"
     ]
    }
   ],
   "source": [
    "#we'll train it for 50 epochs at 3 different learning rates\n",
    "model.nEpochs(50, data, 'mse', .1, 1)\n",
    "model.nEpochs(50, data, 'mse', .05, 1)\n",
    "model.nEpochs(50, data, 'mse', .01, 1)\n",
    "\n",
    "#then, print the slope and intercept!\n",
    "print(model.layersList[0].linear,model.layersList[0].bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b22cf5-daad-465a-b592-184668f453d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
